Preparation:
Here is my system running the following applications: Ubuntu 24.04, CUDA 12.8, RTX A6000, Driver 570. Notice that a docker image shall be applied to ensure that the developer environment is well set: Developer Environment Setup — isaac_ros_docs documentation

1 . RealSense Camera Integration with Isaac ROS

Intel RealSense cameras provide the foundation for robust 3D perception in industrial robotics applications. Our implementation ensures optimal performance for your manufacturing and automation needs.

We begin with a complete system preparation, installing essential dependencies including Git,  USB drivers, and other development tools. The RealSense library is built from source to ensure maximum compatibility and performance optimization for your specific hardware configuration.

After obtaining a physic device of Realsense camera, e.g. 435i (Depth Camera D435i – Intel® RealSense™ Depth and Tracking Cameras), you need to update the firmware to desired version: Firmware Update Tool (rs-fw-update)

We provide a isaas-ros-realsense that can be used in the docker developer interface: https://nvidia-isaac-ros.github.io/getting_started/hardware_setup/sensors/realsense_setup.html. However, I found this difficult to install on my Ubuntu 24.04 desktop, and an alternative way is to install is via the official ros2 build https://github.com/IntelRealSense/realsense-ros

If you choose this customized option to build the ros2 realsense from source, you need to ensure the resolution, depth, and camera information are synchronized during the camera information publishing. The following command is what I am using for all the applications:
ros2 launch realsense2_camera rs_launch.py \ 
    rgb_camera.color_profile:=640x480x30 depth_module.depth_profile:=640x480x30 \
        pointcloud.enable:=false enable_rgbd:=true enable_sync:=true align_depth.enable:=true enable_color:=true enable_depth:=true


2. Object Detection with SyntheticaDETR

FoundationPose (Create, Design, and Deploy Robotics Applications Using New NVIDIA Isaac Foundation Models and Workflows – Robovity) relies on SyntheticaDETR (https://forums.developer.nvidia.com/t/foundationpose-estimation-with-custom-object/312252) because it requires accurate 2D detections or segmentation masks to isolate the object before estimating its 6D pose. In practical pipelines—like in NVIDIA’s Isaac ROS—SyntheticaDETR first identifies and localizes objects via fast and reliable bounding boxes or masks, which are then fed into FoundationPose for refinement and pose estimation Without this detection step, FoundationPose wouldn't know where to start its implicit rendering and pose refinement process—making methods like SyntheticaDETR critical in enabling the end-to-end workflow.
You can get familiar with using SyntheticaDERT IsaacRos from our tutorial (https://nvidia-isaac-ros.github.io/repositories_and_pacakages/isaac_ros_object_detection/isaac_ros_rtdetr/index.html). 



However, the object classes of the SyntheticsDERT may not fully cover what you need (Looking for Object Class IDs in SyntheticaDETR Model - Robotics - Isaac / Isaac ROS - NVIDIA Developer Forums) You may need to fune tune the Real-Time DETR https://github.com/lyuwenyu/RT-DETR from your dataset to obtain the best performance. (This will also influence the FoundationPose Stage)

3. FoundationPose

FoundationPose is deployed via the isaac_ros_foundationpose node, which runs a graph comprising depth estimation, 2D object detection (using SyntheticaDETR or alternatives), and the core FoundationPose model. The process begins with a depth map (e.g., via Isaac ROS ESS) and a 2D bounding box or mask—typically generated by SyntheticaDETR—to isolate the object (catalog.ngc.nvidia.com+15nvidia-isaac-ros.github.io+15nvidia-isaac-ros.github.io+15). The FoundationPose model itself includes two TensorRT-optimized networks: a refine network that iteratively adjusts initial pose hypotheses and a score network that selects the best estimate (nvidia-isaac-ros.github.io+4catalog.ngc.nvidia.com+4github.com+4). The node is configured with ROS parameters to load the object's 3D mesh and textures, as well as the paths to the ONNX or TensorRT engine plan files for each model . Once the initial pose is obtained, the system transitions to a tracking mode, where only the refine network runs—leveraging the previous frame’s pose to achieve high-speed (> 120 FPS on Jetson Orin) update cycles



Several key factors can influence the performance of FoundationPose. First, the quality of the depth input—such as a RealSense depth map—directly affects pose accuracy, since noisy or biased depth measurements compromise the registration and refinement steps. Similarly, the accuracy of object segmentation (e.g., from SyntheticaDETR or CNOS/SAM) is critical: jagged or imprecise masks can mislead the refine-and-score networks, especially during initialization or tracking foundationpose.github.io+2arxiv.org+2deepwiki.com+2. Next, the fidelity of the 3D model—whether it’s a CAD model or a neural implicit reconstruction—matters: while FoundationPose shows resilience to small geometric inaccuracies, texture quality and surface detail still play a role in overall accuracy . On the software side, TensorRT optimization, mixed‑precision rendering, and efficient GPU batching Finally, tracking performance hinges on frame-to-frame consistency and smooth camera/object motion—when the object rapidly exits the field of view or segmentation fails, the refine-only tracking mode may struggle.


Source code

For you references, I will organize my code (not done yet) and detailed instructions to a Github repo: yizhouzhao-nvidia/IsaacRosBestPractice: Best Practices for Isaac ROS
